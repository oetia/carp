{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import cupy as np\n",
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "\n",
    "  def __init__(self, list: list | np.ndarray) -> None:\n",
    "    self.name: str = \"\"\n",
    "    self.values: np.ndarray = np.array(list, dtype=\"float32\")\n",
    "    self.parents: tuple[Tensor, Tensor] = (None, None)\n",
    "    self.gradient: np.array = np.zeros_like(self.values)\n",
    "    self._backward: function = lambda *args: None\n",
    "    self.visited = False\n",
    "\n",
    "  def __add__(self, other: Tensor) -> Tensor:\n",
    "    child = Tensor(self.values + other.values)\n",
    "    child.parents = (self, other)\n",
    "    def _backward() -> None:\n",
    "      self.gradient += np.ones_like(self.values) * child.gradient\n",
    "      other.gradient += np.ones_like(other.values) * child.gradient\n",
    "    child._backward = _backward\n",
    "    return child\n",
    "  \n",
    "  def __sub__(self, other: Tensor) -> Tensor:\n",
    "    return self.__add__(-other)\n",
    "\n",
    "  def __mul__(self, other: Tensor) -> Tensor:\n",
    "    child = Tensor(self.values * other.values)\n",
    "    child.parents = (self, other)\n",
    "    def _backward() -> None:\n",
    "      self.gradient += other.values * child.gradient # mistake made using Tensor object instead of tensor value\n",
    "      other.gradient += self.values * child.gradient # keep in mind whether dealing with object object.values or object.gradient\n",
    "    child._backward = _backward\n",
    "    return child\n",
    "\n",
    "  def __truediv__(self, other: Tensor) -> Tensor:\n",
    "    return self.__mul__(other ** -1)\n",
    "\n",
    "  def __neg__(self) -> Tensor:\n",
    "    child = Tensor(-self.values)\n",
    "    child.parents = (self, None)\n",
    "    def _backward():\n",
    "      self.gradient += -np.ones_like(self.values) * child.gradient\n",
    "    child._backward = _backward\n",
    "    return child\n",
    "\n",
    "  def __pow__(self, other: float) -> Tensor:\n",
    "    child = Tensor(self.values ** other)\n",
    "    child.parents = (self, None)\n",
    "    def _backward():\n",
    "      self.gradient += other * self.values ** (other - 1) * child.gradient\n",
    "    child._backward = _backward\n",
    "    return child\n",
    "  \n",
    "  def __matmul__(self, other: Tensor) -> Tensor:\n",
    "    child = Tensor(self.values @ other.values)\n",
    "    child.parents = (self, other)\n",
    "    def _backward():\n",
    "      # self: A x B - other B x C - child A x C\n",
    "      self.gradient = child.gradient @ other.values.T # A x C @ C x B => A x B\n",
    "      other.gradient = self.values.T @ child.gradient # B x A @ A x C => B x C\n",
    "    child._backward = _backward\n",
    "    return child\n",
    "\n",
    "  def exp(self) -> Tensor:\n",
    "    child = Tensor(np.exp(self.values))\n",
    "    child.parents = (self, None)\n",
    "    def _backward():\n",
    "      self.gradient = child.values * child.gradient\n",
    "    child._backward = _backward\n",
    "    return child\n",
    "\n",
    "  def tanh(self) -> Tensor:\n",
    "    child = Tensor(np.tanh(self.values))\n",
    "    child.parents = (self, None)\n",
    "    def _backward():\n",
    "      self.gradient += (1 - child.values ** 2) * child.gradient\n",
    "    child._backward = _backward\n",
    "    return child\n",
    "\n",
    "  def sum(self) -> Tensor:\n",
    "    child = Tensor([self.values.sum()])\n",
    "    child.parents = (self, None)\n",
    "    def _backward() -> None:\n",
    "      self.gradient += np.ones_like(self.values) * child.gradient\n",
    "    child._backward = _backward\n",
    "    return child\n",
    "\n",
    "  def topological_sort(self) -> list[Tensor]:\n",
    "    dfs_sort: list[Tensor] = []\n",
    "    def dfs(node: Tensor):\n",
    "      if node.parents[0]:\n",
    "        dfs(node.parents[0])\n",
    "      if node.parents[1]:\n",
    "        dfs(node.parents[1])\n",
    "      if not node.visited:\n",
    "        dfs_sort.append(node); node.visited = True\n",
    "    dfs(self); return list(reversed(dfs_sort))\n",
    "\n",
    "  def backward(self):\n",
    "    self.gradient = np.ones_like(self.values) # gradient w.r.t self\n",
    "    for node in self.topological_sort():\n",
    "      node._backward()\n",
    "      # print(node)\n",
    "\n",
    "  def zero_grad(self):\n",
    "    self.gradient = np.zeros_like(self.values)\n",
    "    self.visited = False\n",
    "  \n",
    "  def __repr__(self) -> str:\n",
    "    return f\"{self.name}: {self.values} : {self.gradient}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": [1.5470199e+10] : [0.]\n",
      "[[1.3207467e+03 3.4329600e+03 3.5435869e+03 ... 2.8773391e+03\n",
      "  5.5576836e+03 3.8428843e+03]\n",
      " [3.8589172e+02 9.3177711e+01 1.3299084e+02 ... 4.2933572e+02\n",
      "  2.6310379e+02 3.9735739e+02]\n",
      " [7.1169453e+04 7.6358018e+03 8.9608062e+04 ... 7.1529531e+04\n",
      "  6.4949184e+04 6.8439097e+03]\n",
      " ...\n",
      " [5.1556559e+04 3.0674984e+05 5.8992612e+05 ... 1.6457391e+05\n",
      "  6.2774131e+05 7.4499312e+05]\n",
      " [3.4821860e+02 2.7197885e+02 2.9004333e+02 ... 2.2436359e+02\n",
      "  2.2142862e+02 2.4646594e+02]\n",
      " [8.1866264e+01 6.8387070e+01 9.1307175e+01 ... 8.9380768e+01\n",
      "  9.2049667e+01 9.0049286e+01]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "a = Tensor(np.random.uniform(size=(500, 1000)))\n",
    "b = Tensor(np.random.uniform(size=(1000, 20)))\n",
    "c = Tensor(np.random.uniform(size=(500, 20)))\n",
    "\n",
    "d = a @ b\n",
    "e = d / c ** 2\n",
    "l = e.sum()\n",
    "print(l)\n",
    "l.backward()\n",
    "print(a.gradient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3207e+03, 3.4330e+03, 3.5436e+03,  ..., 2.8773e+03, 5.5577e+03,\n",
      "         3.8429e+03],\n",
      "        [3.8589e+02, 9.3178e+01, 1.3299e+02,  ..., 4.2934e+02, 2.6310e+02,\n",
      "         3.9736e+02],\n",
      "        [7.1169e+04, 7.6358e+03, 8.9608e+04,  ..., 7.1530e+04, 6.4949e+04,\n",
      "         6.8439e+03],\n",
      "        ...,\n",
      "        [5.1557e+04, 3.0675e+05, 5.8993e+05,  ..., 1.6457e+05, 6.2774e+05,\n",
      "         7.4499e+05],\n",
      "        [3.4822e+02, 2.7198e+02, 2.9004e+02,  ..., 2.2436e+02, 2.2143e+02,\n",
      "         2.4647e+02],\n",
      "        [8.1866e+01, 6.8387e+01, 9.1307e+01,  ..., 8.9381e+01, 9.2050e+01,\n",
      "         9.0049e+01]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "np.random.seed(42)\n",
    "a = torch.tensor(np.random.uniform(size=(500, 1000)), requires_grad=True)\n",
    "b = torch.tensor(np.random.uniform(size=(1000, 20)), requires_grad=True)\n",
    "c = torch.tensor(np.random.uniform(size=(500, 20)), requires_grad=True)\n",
    "\n",
    "d = a @ b; d.retain_grad()\n",
    "e = d / c ** 2; e.retain_grad()\n",
    "l = e.sum(); l.retain_grad()\n",
    "l.backward()\n",
    "# print(l)\n",
    "# tensors = list(reversed([a, b, c, d, e, l]))\n",
    "# for tensor in tensors:\n",
    "#   print(tensor)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l: [129.51688] : [1.]\n",
      "f: [10.775167 37.339153 81.40256 ] : [1. 1. 1.]\n",
      ": [1.3130352 1.0373148 1.0049698] : [ 8.206305 35.99597  81.      ]\n",
      ": [0.7615942 0.9640276 0.9950548] : [-14.148174 -38.73245  -81.80711 ]\n",
      "e2: [ 8.206305 35.99597  81.      ] : [1.3130352 1.0373148 1.0049698]\n",
      "e: [-2.8646648 -5.9996643 -9.       ] : [ -7.522812 -12.447081 -18.089457]\n",
      ": [-3. -6. -9.] : [ -7.522812 -12.447081 -18.089457]\n",
      "c: [3. 6. 9.] : [ 7.522812 12.447081 18.089457]\n",
      ": [1.3533528e-01 3.3546262e-04 1.5229981e-08] : [ -7.522812 -12.447081 -18.089457]\n",
      "dn: [ -2.  -8. -18.] : [-1.0181018e+00 -4.1755303e-03 -2.7550209e-07]\n",
      "d: [ 2.  8. 18.] : [1.0181018e+00 4.1755303e-03 2.7550209e-07]\n",
      "b: [2. 4. 6.] : [1.0181018e+00 8.3510606e-03 8.2650627e-07]\n",
      "a: [1. 2. 3.] : [-3.9056666  -2.7197769  -0.80710727]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([1, 2, 3]); a.name = \"a\"\n",
    "b = Tensor([2, 4, 6]); b.name = \"b\"\n",
    "c = Tensor([3, 6, 9]); c.name = \"c\"\n",
    "d = a * b; d.name = \"d\"\n",
    "dn = -d; dn.name = \"dn\"\n",
    "e = dn.exp() - c; e.name = \"e\"\n",
    "e2 = e ** 2; e2.name = \"e2\"\n",
    "f = e2 / a.tanh(); f.name = \"f\"\n",
    "l = f.sum(); l.name = \"l\"\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing with pytorch\n",
    "import torch\n",
    "a = torch.tensor([1, 2, 3], dtype=float, requires_grad=True)\n",
    "b = torch.tensor([2, 4, 6], dtype=float, requires_grad=True)\n",
    "c = torch.tensor([3, 6, 9], dtype=float, requires_grad=True)\n",
    "d = a * b; d.retain_grad()\n",
    "dn = -d; dn.retain_grad()\n",
    "e = dn.exp() - c; e.retain_grad()\n",
    "e2 = e ** 2; e2.retain_grad()\n",
    "f = e2 / a.tanh(); f.retain_grad()\n",
    "l = f.sum(); l.retain_grad()\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., dtype=torch.float64)\n",
      "tensor([1., 1., 1.], dtype=torch.float64)\n",
      "tensor([1.3130, 1.0373, 1.0050], dtype=torch.float64)\n",
      "tensor([ -7.5228, -12.4471, -18.0895], dtype=torch.float64)\n",
      "tensor([1.0181e+00, 4.1755e-03, 2.7550e-07], dtype=torch.float64)\n",
      "tensor([ 7.5228, 12.4471, 18.0895], dtype=torch.float64)\n",
      "tensor([1.0181e+00, 8.3511e-03, 8.2651e-07], dtype=torch.float64)\n",
      "tensor([-3.9057, -2.7198, -0.8071], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "tensors = list(reversed([a, b, c, d, e, e2, f, l]))\n",
    "for tensor in tensors:\n",
    "  print(tensor.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(968.7626, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "tensor([ 32.8259, 203.3137, 732.6230], dtype=torch.float64,\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([ 25., 196., 729.], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "tensor([ -5., -14., -27.], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "tensor([ 2.,  8., 18.], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([3., 6., 9.], dtype=torch.float64, requires_grad=True)\n",
      "tensor([2., 4., 6.], dtype=torch.float64, requires_grad=True)\n",
      "tensor([1., 2., 3.], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "tensors = list(reversed([a, b, c, d, e, e2, f, l]))\n",
    "for tensor in tensors:\n",
    "  print(tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
