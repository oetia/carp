{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cupy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_gpu = cp.array([1, 2, 3])\n",
    "x_cpu = x_gpu.get()\n",
    "type(x_cpu)\n",
    "type(x_gpu)\n",
    "\n",
    "# lets keep things on purely on the cpu for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "  def __init__(self, list: list) -> None:\n",
    "    self.name: str = \"\"\n",
    "    self.values: np.array = np.array(list, dtype=\"float32\")\n",
    "    self.parents: tuple[Tensor, Tensor] = (None, None)\n",
    "    self.gradient: np.array = np.zeros_like(self.values)\n",
    "    self._backward: function = lambda *args: None\n",
    "    self.visited = False\n",
    "\n",
    "  def __add__(self, other: Tensor) -> Tensor:\n",
    "    child = Tensor(self.values + other.values)\n",
    "    child.parents = (self, other)\n",
    "    def _backward() -> None:\n",
    "      self.gradient += np.ones_like(self.values) * child.gradient\n",
    "      other.gradient += np.ones_like(other.values) * child.gradient\n",
    "    child._backward = _backward\n",
    "    return child\n",
    "  \n",
    "  def __sub__(self, other: Tensor) -> Tensor:\n",
    "    return self.__add__(-other)\n",
    "\n",
    "  def __mul__(self, other: Tensor) -> Tensor:\n",
    "    child = Tensor(self.values * other.values)\n",
    "    child.parents = (self, other)\n",
    "    def _backward() -> None:\n",
    "      self.gradient += other.values * child.gradient # mistake made using Tensor object instead of tensor value\n",
    "      other.gradient += self.values * child.gradient # keep in mind whether dealing with object object.values or object.gradient\n",
    "    child._backward = _backward\n",
    "    return child\n",
    "\n",
    "  def __truediv__(self, other: Tensor) -> Tensor:\n",
    "    return self.__mul__(other ** -1)\n",
    "\n",
    "  def __neg__(self) -> Tensor:\n",
    "    child = Tensor(-self.values)\n",
    "    child.parents = (self, None)\n",
    "    def _backward():\n",
    "      self.gradient += -np.ones_like(self.values) * child.gradient\n",
    "    child._backward = _backward\n",
    "    return child\n",
    "\n",
    "  def __pow__(self, other: float) -> Tensor:\n",
    "    child = Tensor(self.values ** other)\n",
    "    child.parents = (self, None)\n",
    "    def _backward():\n",
    "      self.gradient += other * self.values ** (other - 1) * child.gradient\n",
    "    child._backward = _backward\n",
    "    return child\n",
    "\n",
    "  def exp(self) -> Tensor:\n",
    "    child = Tensor(np.exp(self.values))\n",
    "    child.parents = (self, None)\n",
    "    def _backward():\n",
    "      self.gradient = child.values * child.gradient\n",
    "    child._backward = _backward\n",
    "    return child\n",
    "\n",
    "  def tanh(self) -> Tensor:\n",
    "    child = Tensor(np.tanh(self.values))\n",
    "    child.parents = (self, None)\n",
    "    def _backward():\n",
    "      self.gradient += (1 - child.values ** 2) * child.gradient\n",
    "    child._backward = _backward\n",
    "    return child\n",
    "\n",
    "  def sum(self) -> Tensor:\n",
    "    child = Tensor([self.values.sum()])\n",
    "    child.parents = (self, None)\n",
    "    def _backward() -> None:\n",
    "      self.gradient += np.ones_like(self.values) * child.gradient\n",
    "    child._backward = _backward\n",
    "    return child\n",
    "\n",
    "  def topological_sort(self) -> list[Tensor]:\n",
    "    dfs_sort: list[Tensor] = []\n",
    "    def dfs(node: Tensor):\n",
    "      if node.parents[0]:\n",
    "        dfs(node.parents[0])\n",
    "      if node.parents[1]:\n",
    "        dfs(node.parents[1])\n",
    "      if not node.visited:\n",
    "        dfs_sort.append(node); node.visited = True\n",
    "    dfs(self); return list(reversed(dfs_sort))\n",
    "\n",
    "  def backward(self):\n",
    "    self.gradient = np.ones_like(self.values) # gradient w.r.t self\n",
    "    for node in self.topological_sort():\n",
    "      node._backward()\n",
    "      print(node)\n",
    "\n",
    "  def zero_grad(self):\n",
    "    self.gradient = np.zeros_like(self.values)\n",
    "    self.visited = False\n",
    "  \n",
    "  def __repr__(self) -> str:\n",
    "    return f\"{self.name}: {self.values} : {self.gradient}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l: [129.51688] : [1.]\n",
      "f: [10.775167 37.339153 81.40256 ] : [1. 1. 1.]\n",
      ": [1.3130352 1.0373148 1.0049698] : [ 8.206305 35.99597  81.      ]\n",
      ": [0.7615942 0.9640276 0.9950548] : [-14.148174 -38.73245  -81.80711 ]\n",
      "e2: [ 8.206305 35.99597  81.      ] : [1.3130352 1.0373148 1.0049698]\n",
      "e: [-2.8646648 -5.9996643 -9.       ] : [ -7.522812 -12.447081 -18.089457]\n",
      ": [-3. -6. -9.] : [ -7.522812 -12.447081 -18.089457]\n",
      "c: [3. 6. 9.] : [ 7.522812 12.447081 18.089457]\n",
      ": [1.3533528e-01 3.3546262e-04 1.5229981e-08] : [ -7.522812 -12.447081 -18.089457]\n",
      "dn: [ -2.  -8. -18.] : [-1.0181018e+00 -4.1755303e-03 -2.7550209e-07]\n",
      "d: [ 2.  8. 18.] : [1.0181018e+00 4.1755303e-03 2.7550209e-07]\n",
      "b: [2. 4. 6.] : [1.0181018e+00 8.3510606e-03 8.2650627e-07]\n",
      "a: [1. 2. 3.] : [-3.9056666  -2.7197769  -0.80710727]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([1, 2, 3]); a.name = \"a\"\n",
    "b = Tensor([2, 4, 6]); b.name = \"b\"\n",
    "c = Tensor([3, 6, 9]); c.name = \"c\"\n",
    "d = a * b; d.name = \"d\"\n",
    "dn = -d; dn.name = \"dn\"\n",
    "e = dn.exp() - c; e.name = \"e\"\n",
    "e2 = e ** 2; e2.name = \"e2\"\n",
    "f = e2 / a.tanh(); f.name = \"f\"\n",
    "l = f.sum(); l.name = \"l\"\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing with pytorch\n",
    "import torch\n",
    "a = torch.tensor([1, 2, 3], dtype=float, requires_grad=True)\n",
    "b = torch.tensor([2, 4, 6], dtype=float, requires_grad=True)\n",
    "c = torch.tensor([3, 6, 9], dtype=float, requires_grad=True)\n",
    "d = a * b; d.retain_grad()\n",
    "dn = -d; dn.retain_grad()\n",
    "e = dn.exp() - c; e.retain_grad()\n",
    "e2 = e ** 2; e2.retain_grad()\n",
    "f = e2 / a.tanh(); f.retain_grad()\n",
    "l = f.sum(); l.retain_grad()\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., dtype=torch.float64)\n",
      "tensor([1., 1., 1.], dtype=torch.float64)\n",
      "tensor([1.3130, 1.0373, 1.0050], dtype=torch.float64)\n",
      "tensor([ -7.5228, -12.4471, -18.0895], dtype=torch.float64)\n",
      "tensor([1.0181e+00, 4.1755e-03, 2.7550e-07], dtype=torch.float64)\n",
      "tensor([ 7.5228, 12.4471, 18.0895], dtype=torch.float64)\n",
      "tensor([1.0181e+00, 8.3511e-03, 8.2651e-07], dtype=torch.float64)\n",
      "tensor([-3.9057, -2.7198, -0.8071], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "tensors = list(reversed([a, b, c, d, e, e2, f, l]))\n",
    "for tensor in tensors:\n",
    "  print(tensor.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(968.7626, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "tensor([ 32.8259, 203.3137, 732.6230], dtype=torch.float64,\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([ 25., 196., 729.], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "tensor([ -5., -14., -27.], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "tensor([ 2.,  8., 18.], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([3., 6., 9.], dtype=torch.float64, requires_grad=True)\n",
      "tensor([2., 4., 6.], dtype=torch.float64, requires_grad=True)\n",
      "tensor([1., 2., 3.], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "tensors = list(reversed([a, b, c, d, e, e2, f, l]))\n",
    "for tensor in tensors:\n",
    "  print(tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
