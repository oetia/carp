{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cupy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_gpu = cp.array([1, 2, 3])\n",
    "x_cpu = x_gpu.get()\n",
    "type(x_cpu)\n",
    "type(x_gpu)\n",
    "\n",
    "# lets keep things on purely on the cpu for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "  def __init__(self, list: list) -> None:\n",
    "    self.name: str = \"\"\n",
    "    self.values: np.array = np.array(list, dtype=\"float32\")\n",
    "    self.parents: tuple[Tensor, Tensor] = (None, None)\n",
    "    self.gradient: np.array = np.zeros_like(self.values)\n",
    "    self._backward: function = lambda *args: None\n",
    "    self.visited = False\n",
    "\n",
    "  def __add__(self, other: Tensor) -> Tensor:\n",
    "    child = Tensor(self.values + other.values)\n",
    "    child.parents = (self, other)\n",
    "    def _backward() -> None:\n",
    "      self.gradient += np.ones_like(self.values) * child.gradient\n",
    "      other.gradient += np.ones_like(other.values) * child.gradient\n",
    "    child._backward = _backward\n",
    "    return child\n",
    "  \n",
    "  def __sub__(self, other: Tensor) -> Tensor:\n",
    "    return self.__add__(-other)\n",
    "\n",
    "  def __mul__(self, other: Tensor) -> Tensor:\n",
    "    child = Tensor(self.values * other.values)\n",
    "    child.parents = (self, other)\n",
    "    def _backward() -> None:\n",
    "      self.gradient += other.values * child.gradient # mistake made using Tensor object instead of tensor value\n",
    "      other.gradient += self.values * child.gradient # keep in mind whether dealing with object object.values or object.gradient\n",
    "    child._backward = _backward\n",
    "    return child\n",
    "\n",
    "  def __truediv__(self, other: Tensor) -> Tensor:\n",
    "    return self.__mul__(other ** -1)\n",
    "\n",
    "  def __neg__(self) -> Tensor:\n",
    "    child = Tensor(-self.values)\n",
    "    child.parents = (self, None)\n",
    "    def _backward():\n",
    "      self.gradient += -np.ones_like(self.values) * child.gradient\n",
    "    child._backward = _backward\n",
    "    return child\n",
    "\n",
    "  def __pow__(self, other: float) -> Tensor:\n",
    "    child = Tensor(self.values ** other)\n",
    "    child.parents = (self, None)\n",
    "    def _backward():\n",
    "      self.gradient += other * self.values ** (other - 1) * child.gradient\n",
    "    child._backward = _backward\n",
    "    return child\n",
    "\n",
    "  def sum(self) -> Tensor:\n",
    "    child = Tensor([self.values.sum()])\n",
    "    child.parents = (self, None)\n",
    "    def _backward() -> None:\n",
    "      self.gradient += np.ones_like(self.values) * child.gradient\n",
    "    child._backward = _backward\n",
    "    return child\n",
    "  \n",
    "  def topological_sort(self) -> list[Tensor]:\n",
    "    dfs_sort: list[Tensor] = []\n",
    "    def dfs(node: Tensor):\n",
    "      if node.parents[0]:\n",
    "        dfs(node.parents[0])\n",
    "      if node.parents[1]:\n",
    "        dfs(node.parents[1])\n",
    "      if not node.visited:\n",
    "        dfs_sort.append(node); node.visited = True\n",
    "    dfs(self); return list(reversed(dfs_sort))\n",
    "\n",
    "  def backward(self):\n",
    "    self.gradient = np.ones_like(self.values) # gradient w.r.t self\n",
    "    for node in self.topological_sort():\n",
    "      node._backward()\n",
    "      print(node)\n",
    "\n",
    "  def zero_grad(self):\n",
    "    self.gradient = np.zeros_like(self.values)\n",
    "    self.visited = False\n",
    "  \n",
    "  def __repr__(self) -> str:\n",
    "    return f\"{self.name}: {self.values} : {self.gradient}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l: [366.] : [1.]\n",
      "f: [ 25.  98. 243.] : [1. 1. 1.]\n",
      ": [1.         0.5        0.33333334] : [ 25. 196. 729.]\n",
      "e2: [ 25. 196. 729.] : [1.         0.5        0.33333334]\n",
      "e: [ -5. -14. -27.] : [-10. -14. -18.]\n",
      ": [-3. -6. -9.] : [-10. -14. -18.]\n",
      "c: [3. 6. 9.] : [10. 14. 18.]\n",
      "dn: [ -2.  -8. -18.] : [-10. -14. -18.]\n",
      "d: [ 2.  8. 18.] : [10. 14. 18.]\n",
      "b: [2. 4. 6.] : [10. 28. 54.]\n",
      "a: [1. 2. 3.] : [-5.  7. 27.]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([1, 2, 3]); a.name = \"a\"\n",
    "b = Tensor([2, 4, 6]); b.name = \"b\"\n",
    "c = Tensor([3, 6, 9]); c.name = \"c\"\n",
    "d = a * b; d.name = \"d\"\n",
    "dn = -d; dn.name = \"dn\"\n",
    "e = dn - c; e.name = \"e\"\n",
    "e2 = e ** 2; e2.name = \"e2\"\n",
    "f = e2 / a; f.name = \"f\"\n",
    "l = f.sum(); l.name = \"l\"\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing with pytorch\n",
    "import torch\n",
    "a = torch.tensor([1, 2, 3], dtype=float, requires_grad=True)\n",
    "b = torch.tensor([2, 4, 6], dtype=float, requires_grad=True)\n",
    "c = torch.tensor([3, 6, 9], dtype=float, requires_grad=True)\n",
    "d = a * b; d.retain_grad()\n",
    "dn = -d; dn.retain_grad()\n",
    "e = dn - c; e.retain_grad()\n",
    "e2 = e ** 2; e2.retain_grad()\n",
    "f = e2 / a; f.retain_grad()\n",
    "l = f.sum(); l.retain_grad()\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., dtype=torch.float64)\n",
      "tensor([1., 1., 1.], dtype=torch.float64)\n",
      "tensor([1.0000, 0.5000, 0.3333], dtype=torch.float64)\n",
      "tensor([-10., -14., -18.], dtype=torch.float64)\n",
      "tensor([10., 14., 18.], dtype=torch.float64)\n",
      "tensor([10., 14., 18.], dtype=torch.float64)\n",
      "tensor([10., 28., 54.], dtype=torch.float64)\n",
      "tensor([-5.,  7., 27.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "tensors = list(reversed([a, b, c, d, e, e2, f, l]))\n",
    "for tensor in tensors:\n",
    "  print(tensor.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2604., dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "tensor([  25.,  392., 2187.], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([ 25., 196., 729.], dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "tensor([ -5., -14., -27.], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "tensor([ 2.,  8., 18.], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([3., 6., 9.], dtype=torch.float64, requires_grad=True)\n",
      "tensor([2., 4., 6.], dtype=torch.float64, requires_grad=True)\n",
      "tensor([1., 2., 3.], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "tensors = list(reversed([a, b, c, d, e, e2, f, l]))\n",
    "for tensor in tensors:\n",
    "  print(tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
